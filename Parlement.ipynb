{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HdOJ8c8Y6ZGv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "from PyPDF2 import PdfReader\n",
        "from datetime import datetime \n",
        "import dateparser\n",
        "import re\n",
        "import locale\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZvnsUL28q9G"
      },
      "source": [
        "# Extracting the text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "V_7X4bAJ9Pca"
      },
      "outputs": [],
      "source": [
        "def extract_pdf_to_txt(src_dir, dest_dir):\n",
        "    # List all files in the source directory\n",
        "    all_files = os.listdir(src_dir)\n",
        "\n",
        "    # Filter PDF files\n",
        "    pdf_files = [f for f in all_files if f.lower().endswith('.pdf')]\n",
        "\n",
        "    # Loop through all the PDF files\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(src_dir, pdf_file)\n",
        "        reader = PdfReader(pdf_path)\n",
        "\n",
        "        # Extract text from the first page\n",
        "        first_page_text = reader.pages[0].extract_text()\n",
        "\n",
        "        # Search for a date in the format \"DD MONTH YYYY\" using a regular expression\n",
        "        date_match = re.search(r'\\d{1,2} (JANVIER|FÉVRIER|MARS|AVRIL|MAI|JUIN|JUILLET|AOÛT|SEPTEMBRE|OCTOBRE|NOVEMBRE|DÉCEMBRE)  \\d{4}', first_page_text)\n",
        "\n",
        "        # If a date is found, use it as a filename\n",
        "        if date_match:\n",
        "            # Get the matched date string\n",
        "            date_str = date_match.group()\n",
        "\n",
        "            # Remove extra spaces from the date string\n",
        "            date_str = ' '.join(date_str.split())\n",
        "\n",
        "            # Convert the date string to a datetime object\n",
        "            date = dateparser.parse(date_str, languages=['fr'])\n",
        "\n",
        "            # Format the date as \"YY-MM-DD\"\n",
        "            formatted_date = date.strftime('%y-%m-%d')\n",
        "\n",
        "            # Create a filename using the formatted date\n",
        "            filename = f\"{formatted_date}.txt\"\n",
        "        # If a date is not found, use the original PDF filename as a base\n",
        "        else:\n",
        "            filename = f\"{os.path.splitext(pdf_file)[0]}.txt\"\n",
        "\n",
        "        txt_path = os.path.join(dest_dir, filename)\n",
        "\n",
        "        # Open the output file for writing with 'utf-8' encoding\n",
        "        with open(txt_path, \"w\", encoding='utf-8') as f:\n",
        "            # Loop through all the pages in the PDF\n",
        "            for page in reader.pages:\n",
        "                # Extract text from the current page and write it to the output file\n",
        "                f.write(page.extract_text())\n",
        "\n",
        "src_dir = r'C:\\Users\\Stephanie\\Documents\\GitHub\\NLP_Parlement\\pdf'\n",
        "dest_dir = r'C:\\Users\\Stephanie\\Documents\\GitHub\\NLP_Parlement\\txt'\n",
        "extract_pdf_to_txt(src_dir, dest_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZRPFKQlA9NZ"
      },
      "source": [
        "# Important information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "parlementarians = pd.read_csv(r'C:\\Users\\Stephanie\\Documents\\GitHub\\NLP_Parlement\\2022.07.22_Parlementaires PFWB_0.csv', header=None)\n",
        "column_names = ['Prénom', 'nom', 'sexe', 'résidence', 'naissance', 'date', 'parti', 'titre']\n",
        "parlementarians.columns = column_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "known_speakers = parlementarians.apply(lambda row: {\"name\": row[\"Prénom\"] + \" \" + row[\"nom\"], \"title\": row[\"titre\"], 'parti': row['parti']}, axis=1).to_list()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EybferjhR1Hq"
      },
      "source": [
        "# Convert to JSON file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "DbTAYxHpSKzf"
      },
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "\n",
        "# Define regular expressions and helper function to parse questions, speakers and statements\n",
        "def find_starting_point(text, starting_sentence='La séance est ouverte'):\n",
        "    starting_point = text.find(starting_sentence)\n",
        "    if starting_point != -1:\n",
        "        return starting_point + len(starting_sentence)\n",
        "    return None\n",
        "\n",
        "# Start the text after the table des matieres \n",
        "starting_point = find_starting_point(full_text)\n",
        "if starting_point is not None:\n",
        "    transcript_text = full_text[starting_point:]\n",
        "else:\n",
        "    print(\"Starting sentence not found. Analyzing the full text.\")\n",
        "    transcript_text = full_text\n",
        "\n",
        "def clean_line_breaks(text):\n",
        "    # Replace hyphenated line breaks with an empty string\n",
        "    cleaned_text = re.sub(r'-\\s+', '', text)\n",
        "    # Clean the text from the \\n characters \n",
        "    cleaned_text = cleaned_text.replace('\\n', ' ')\n",
        "    return cleaned_text\n",
        "\n",
        "transcript_text = clean_line_breaks(transcript_text)\n",
        "\n",
        "\n",
        "def find_questions(text):\n",
        "    question_pattern = re.compile(r'(?<=\\d\\.\\d Question)(.*?)(?=\\d\\.\\d Question)', re.DOTALL)\n",
        "    return question_pattern.findall(text)\n",
        "\n",
        "def find_projets(text):\n",
        "    projet_pattern = re.compile(r'\\d+ Projet de décret(.*?)(?=\\d+ Projet de décret|$)', re.DOTALL)\n",
        "    return projet_pattern.findall(text)\n",
        "\n",
        "def process_transcript(text, known_speakers):\n",
        "    # Create a regex pattern for speaker names\n",
        "    speaker_pattern = r'\\b(?:' + '|'.join([re.escape(speaker[\"name\"]) for speaker in known_speakers]) + r')\\b'\n",
        "\n",
        "    # Split the text into parts\n",
        "    parts = re.split('(' + speaker_pattern + ')', text)\n",
        "    # Initialize the result list\n",
        "    result = []\n",
        "\n",
        "    # Initialize the current speaker\n",
        "    current_speaker = None\n",
        "\n",
        "    # Iterate through the parts\n",
        "    for part in parts:\n",
        "        found_speaker = False\n",
        "        for speaker in known_speakers:\n",
        "            if speaker[\"name\"] == part.strip():\n",
        "                current_speaker = speaker\n",
        "                found_speaker = True\n",
        "                break\n",
        "\n",
        "        if not found_speaker and current_speaker is not None:\n",
        "            # Append the spoken text by the current speaker\n",
        "            spoken_text = part.strip()\n",
        "            if spoken_text:\n",
        "                result.append({\"speaker\": current_speaker[\"name\"], \"title\": current_speaker[\"title\"], \"text\": spoken_text})\n",
        "\n",
        "    return result\n",
        "\n",
        "def create_json_file(questions_data, projets_data, filename):\n",
        "    data = {\n",
        "        \"questions\": questions_data,\n",
        "        \"projets\": projets_data,\n",
        "    }\n",
        "    with open(filename, \"w\", encoding='utf-8') as outfile:\n",
        "        json.dump(data, outfile, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_file(file_path, known_speakers):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        full_text = f.read()\n",
        "\n",
        "    starting_point = find_starting_point(full_text)\n",
        "    if starting_point is not None:\n",
        "        transcript_text = full_text[starting_point:]\n",
        "    else:\n",
        "        print(\"Starting sentence not found. Analyzing the full text.\")\n",
        "        transcript_text = full_text\n",
        "\n",
        "    transcript_text = transcript_text.replace('\\n', ' ')\n",
        "    transcript_text = clean_line_breaks(transcript_text)\n",
        "\n",
        "    date = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    # Find and process questions\n",
        "    questions_text = find_questions(transcript_text)\n",
        "    questions_data = []\n",
        "    for question_text in questions_text:\n",
        "        result = process_transcript(question_text, known_speakers)\n",
        "        question_data = {\n",
        "            \"date\": date,\n",
        "            \"type\": questions_type,\n",
        "            \"theme\": theme,\n",
        "            \"text\": result\n",
        "        }\n",
        "        questions_data.append(question_data)\n",
        "\n",
        "    # Find and process projets\n",
        "    projets_text = find_projets(transcript_text)\n",
        "    projets_data = []\n",
        "    for projet_text in projets_text:\n",
        "        result = process_transcript(projet_text, known_speakers)\n",
        "        projet_data = {\n",
        "            \"date\": date,\n",
        "            \"type\": projets_type,\n",
        "            \"theme\": theme,\n",
        "            \"text\": result\n",
        "        }\n",
        "        projets_data.append(projet_data)\n",
        "\n",
        "    create_json_file(questions_data, projets_data, f\"{date}.json\")\n",
        "\n",
        "    return questions_data, projets_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "questions, projets = process_file(r'C:\\Users\\Stephanie\\Documents\\GitHub\\NLP_Parlement\\txt\\20-05-13.txt', known_speakers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_directory(directory_path, known_speakers):\n",
        "    all_questions_data = []\n",
        "    all_projets_data = []\n",
        "\n",
        "    for file_name in os.listdir(directory_path):\n",
        "        if file_name.endswith('.txt'):\n",
        "            file_path = os.path.join(directory_path, file_name)\n",
        "            questions_data, projets_data = process_file(file_path, known_speakers)\n",
        "            all_questions_data.extend(questions_data)\n",
        "            all_projets_data.extend(projets_data)\n",
        "\n",
        "    create_json_file(all_questions_data, all_projets_data, 'combined.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "process_directory(r'C:\\Users\\Stephanie\\Documents\\GitHub\\NLP_Parlement\\txt', known_speakers)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
