{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HdOJ8c8Y6ZGv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "from PyPDF2 import PdfReader\n",
        "from datetime import datetime \n",
        "import dateparser\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import cv2\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "from dateparser import parse as dateparser\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZvnsUL28q9G"
      },
      "source": [
        "# Extracting the text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "V_7X4bAJ9Pca"
      },
      "outputs": [],
      "source": [
        "def extract_pdf_to_txt(src_dir, dest_dir):\n",
        "    # List all files in the source directory\n",
        "    all_files = os.listdir(src_dir)\n",
        "\n",
        "    # Filter PDF files\n",
        "    pdf_files = [f for f in all_files if f.lower().endswith('.pdf')]\n",
        "\n",
        "    # Loop through all the PDF files\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(src_dir, pdf_file)\n",
        "        reader = PdfReader(pdf_path)\n",
        "\n",
        "        # Extract text from the first page\n",
        "        first_page_text = reader.pages[0].extract_text()\n",
        "\n",
        "        # Search for a date in the format \"DD MONTH YYYY\" using a regular expression\n",
        "        date_match = re.search(r'\\d{1,2} (JANVIER|FÉVRIER|MARS|AVRIL|MAI|JUIN|JUILLET|AOÛT|SEPTEMBRE|OCTOBRE|NOVEMBRE|DÉCEMBRE)  \\d{4}', first_page_text)\n",
        "\n",
        "        # If a date is found, use it as a filename\n",
        "        if date_match:\n",
        "            # Get the matched date string\n",
        "            date_str = date_match.group()\n",
        "\n",
        "            # Remove extra spaces from the date string\n",
        "            date_str = ' '.join(date_str.split())\n",
        "\n",
        "            # Convert the date string to a datetime object\n",
        "            date = dateparser.parse(date_str, languages=['fr'])\n",
        "\n",
        "            # Format the date as \"YY-MM-DD\"\n",
        "            formatted_date = date.strftime('%y-%m-%d')\n",
        "\n",
        "            # Create a filename using the formatted date\n",
        "            filename = f\"{formatted_date}.txt\"\n",
        "        # If a date is not found, use the original PDF filename as a base\n",
        "        else:\n",
        "            filename = f\"{os.path.splitext(pdf_file)[0]}.txt\"\n",
        "\n",
        "        txt_path = os.path.join(dest_dir, filename)\n",
        "\n",
        "        # Open the output file for writing with 'utf-8' encoding\n",
        "        with open(txt_path, \"w\", encoding='utf-8') as f:\n",
        "            # Loop through all the pages in the PDF\n",
        "            for page in reader.pages:\n",
        "                # Extract text from the current page and write it to the output file\n",
        "                f.write(page.extract_text())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using cv2\n",
        "def extract_pdf_to_txt(src_dir, dest_dir):\n",
        "    pytesseract.pytesseract.tesseract_cmd = 'C:/Program Files/Tesseract-OCR/tesseract.exe'\n",
        "\n",
        "    all_files = os.listdir(src_dir)\n",
        "    pdf_files = [f for f in all_files if f.lower().endswith('.pdf')]\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(src_dir, pdf_file)\n",
        "\n",
        "        # Convert PDF pages to images\n",
        "        images = convert_from_path(pdf_path)\n",
        "\n",
        "        # Extract text from the first page\n",
        "        first_page_np = np.array(images[0])\n",
        "        first_page_gray = cv2.cvtColor(first_page_np, cv2.COLOR_BGR2GRAY)\n",
        "        first_page_thresh = cv2.threshold(first_page_gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
        "        first_page_text = pytesseract.image_to_string(first_page_thresh, lang='fra', output_type=pytesseract.Output.STRING)\n",
        "\n",
        "        date_match = re.search(r'\\d{1,2} (JANVIER|FÉVRIER|MARS|AVRIL|MAI|JUIN|JUILLET|AOÛT|SEPTEMBRE|OCTOBRE|NOVEMBRE|DÉCEMBRE) \\d{4}', first_page_text)\n",
        "\n",
        "        if date_match:\n",
        "            date_str = date_match.group()\n",
        "            date_str = ' '.join(date_str.split())\n",
        "            date = dateparser(date_str, languages=['fr'])\n",
        "            formatted_date = date.strftime('%y-%m-%d')\n",
        "            filename = f\"{formatted_date}.txt\"\n",
        "        else:\n",
        "            filename = f\"{os.path.splitext(pdf_file)[0]}.txt\"\n",
        "        \n",
        "        def preprocess_image(image_np):\n",
        "            # Resize the image\n",
        "            height, width = image_np.shape[:2]\n",
        "            resized_image = cv2.resize(image_np, (width * 2, height * 2), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "            # Convert to grayscale\n",
        "            gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Apply Gaussian blur\n",
        "            blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "            # Thresholding\n",
        "            thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
        "\n",
        "            # Dilation and Erosion\n",
        "            kernel = np.ones((1, 1), np.uint8)\n",
        "            dilated = cv2.dilate(thresh, kernel, iterations=1)\n",
        "            eroded = cv2.erode(dilated, kernel, iterations=1)\n",
        "\n",
        "            return eroded\n",
        "\n",
        "        txt_path = os.path.join(dest_dir, filename)\n",
        "\n",
        "        with open(txt_path, \"w\", encoding='utf-8') as f:\n",
        "            for image in images:\n",
        "                image_np = np.array(image)\n",
        "                preprocessed_image = preprocess_image(image_np)\n",
        "                text = pytesseract.image_to_string(preprocessed_image, lang='fra', output_type=pytesseract.Output.STRING)\n",
        "                f.write(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "src_dir = r'C:\\Users\\Stephanie\\Documents\\GitHub\\NLP_Parlement\\pdf'\n",
        "dest_dir = r'C:\\Users\\Stephanie\\Documents\\GitHub\\NLP_Parlement\\txt'\n",
        "extract_pdf_to_txt(src_dir, dest_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZRPFKQlA9NZ"
      },
      "source": [
        "# Important information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "parlementarians = pd.read_csv(r'C:\\Users\\Stephanie\\Documents\\GitHub\\NLP_Parlement\\2022.07.22_Parlementaires PFWB_0.csv', header=None)\n",
        "column_names = ['Prénom', 'nom', 'sexe', 'résidence', 'naissance', 'date', 'parti', 'titre']\n",
        "parlementarians.columns = column_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "known_speakers = parlementarians.apply(lambda row: {\"name\": row[\"Prénom\"] + \" \" + row[\"nom\"], \"title\": row[\"titre\"], 'parti': row['parti']}, axis=1).to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EybferjhR1Hq"
      },
      "source": [
        "# Convert to JSON file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DbTAYxHpSKzf"
      },
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "# Define regular expressions and helper function to parse questions, speakers and statements\n",
        "def find_starting_point(text, starting_sentence='La séance est ouverte'):\n",
        "    starting_point = text.find(starting_sentence)\n",
        "    if starting_point != -1:\n",
        "        return starting_point + len(starting_sentence)\n",
        "    return None\n",
        "\n",
        "def clean_line_breaks(text):\n",
        "    # Replace hyphenated line breaks with an empty string\n",
        "    cleaned_text = re.sub(r'-\\s+', '', text)\n",
        "    # Clean the text from the \\n characters \n",
        "    cleaned_text = cleaned_text.replace('\\n', ' ')\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "def find_questions(text):\n",
        "    question_pattern = re.compile(r'(?<=\\d\\.\\d Question)(.*?)(?=\\d\\.\\d Question)', re.DOTALL)\n",
        "    return question_pattern.findall(text)\n",
        "\n",
        "def find_projets(text):\n",
        "    projet_pattern = re.compile(r'\\d+ Projet de décret(.*?)(?=\\d+ Projet de décret|$)', re.DOTALL)\n",
        "    return projet_pattern.findall(text)\n",
        "\n",
        "def process_transcript(text, known_speakers):\n",
        "    # Create a regex pattern for speaker names\n",
        "    speaker_pattern = r'(?:Mme|M\\.)\\s+(?:' + '|'.join([re.escape(speaker[\"name\"]) for speaker in known_speakers]) + r')\\b(?:\\s*\\([^)]*\\)|\\s*[,.-])'\n",
        "\n",
        "    # Update the regex pattern to account for the space before the speaker's name\n",
        "    speaker_pattern_with_space = r'\\s*(?:' + '|'.join([re.escape(speaker[\"name\"]) for speaker in known_speakers]) + r')\\b'\n",
        "\n",
        "    # Find the starting point of the actual conversation\n",
        "    starting_point_pattern = re.compile(r'Question de .*? à .*?«.*?»', re.DOTALL)\n",
        "    starting_point = starting_point_pattern.search(text)\n",
        "    if starting_point:\n",
        "        text = text[starting_point.end():]\n",
        "\n",
        "    # Extract the theme\n",
        "    theme_pattern = re.compile(r'«(.*?)»')\n",
        "    theme_match = theme_pattern.search(text)\n",
        "    if theme_match:\n",
        "        theme = theme_match.group(1)\n",
        "        # Update the starting point of the text after the theme\n",
        "        text = text[theme_match.end():]\n",
        "    else:\n",
        "        theme = 'TBD'\n",
        "\n",
        "    # Split the text into parts\n",
        "    parts = re.split('(' + speaker_pattern_with_space + ')', text)\n",
        "    # Initialize the result list\n",
        "    result = []\n",
        "\n",
        "\n",
        "    # Initialize the current speaker\n",
        "    current_speaker = None\n",
        "\n",
        "    # Iterate through the parts\n",
        "    for part in parts:\n",
        "        found_speaker = False\n",
        "        for speaker in known_speakers:\n",
        "            if speaker[\"name\"] == part.strip():\n",
        "                current_speaker = speaker\n",
        "                found_speaker = True\n",
        "                break\n",
        "\n",
        "        if not found_speaker and current_speaker is not None:\n",
        "            # Append the spoken text by the current speaker\n",
        "            spoken_text = part.strip()\n",
        "\n",
        "            # Remove text within brackets and following whitespace\n",
        "            spoken_text = re.sub(r'\\(.*?\\)\\s*', '', spoken_text)\n",
        "\n",
        "            if spoken_text:\n",
        "                result.append({\"speaker\": current_speaker[\"name\"], \"title\": current_speaker[\"title\"], \"text\": spoken_text})\n",
        "\n",
        "    return result, theme\n",
        "\n",
        "def create_json_file(questions_data, projets_data, filename):\n",
        "    data = {\n",
        "        \"questions\": questions_data,\n",
        "        \"projets\": projets_data,\n",
        "    }\n",
        "    with open(filename, \"w\", encoding='utf-8') as outfile:\n",
        "        json.dump(data, outfile, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_file(file_path, known_speakers):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        full_text = f.read()\n",
        "\n",
        "    starting_point = find_starting_point(full_text)\n",
        "    if starting_point is not None:\n",
        "        transcript_text = full_text[starting_point:]\n",
        "    else:\n",
        "        print(\"Starting sentence not found. Analyzing the full text.\")\n",
        "        transcript_text = full_text\n",
        "\n",
        "    transcript_text = transcript_text.replace('\\n', ' ')\n",
        "    transcript_text = clean_line_breaks(transcript_text)\n",
        "\n",
        "    date = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    # Find and process questions\n",
        "    questions_text = find_questions(transcript_text)\n",
        "    questions_data = []\n",
        "    for question_text in questions_text:\n",
        "        result, theme = process_transcript(question_text, known_speakers)\n",
        "        question_data = {\n",
        "            \"date\": date,\n",
        "            \"type\": 'question',\n",
        "            \"theme\": theme,\n",
        "            'thematic': 'TBD', \n",
        "            \"text\": result\n",
        "        }\n",
        "        questions_data.append(question_data)\n",
        "\n",
        "    # Find and process projets\n",
        "    projets_text = find_projets(transcript_text)\n",
        "    projets_data = []\n",
        "    for projet_text in projets_text:\n",
        "        result, theme = process_transcript(projet_text, known_speakers)\n",
        "        projet_data = {\n",
        "            \"date\": date,\n",
        "            \"type\": 'projet',\n",
        "            \"theme\": theme,\n",
        "            'thematic': 'TBD', \n",
        "            \"text\": result\n",
        "        }\n",
        "        projets_data.append(projet_data)\n",
        "\n",
        "    create_json_file(questions_data, projets_data, f\"{date}.json\")\n",
        "\n",
        "    return questions_data, projets_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "questions, projets = process_file(r'txt\\23-04-12.txt', known_speakers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'date': '23-04-12',\n",
              " 'type': 'question',\n",
              " 'theme': 'Audit externe de la Ligue belge francophone d’athlétisme',\n",
              " 'thematic': 'TBD',\n",
              " 'text': [{'speaker': 'Mourad Sahli',\n",
              "   'title': 'Député',\n",
              "   'text': \". Le 14 mars dernier, en commission des Sports, Madame la Ministre, je vous ai interrogée sur les dysfonctionnements au sein de la Ligue belge francophone d’athlétisme et, en particulier, sur les difficultés entre certains administrateurs et le président de la LBFA. Vous m'avez alors répondu que, d’après le rapport d’audit, la LBFA entretenait une comptabilité correcte et qu'elle s’inscrivait dans un processus de bonne gouvernance. Je  trouvais votre réponse quelque peu succincte.  À mes yeux, deux points sont importants dans ce dossier: d’une part, le rapport provisoire reçu par votre administration aurait été envoyé à la LBFA pour avoir son avis avant l'élaboration du rapport final et, d’autre part, les lanceurs d'alerte n’auraient pas été auditionnés et n’auraient, par conséquent, pas pu  donner leur avis.  N’est-il pas étrange que ce rapport d’audit provisoire soit remis à la partie qui fait l’objet d’un audit afin d’obtenir son avis? Cela ne signifie-t-il pas de lui demander une autoévaluation? Cela me semble contraire au but d’un audit externe et indépendant? Disposez-vous des conclusions de ce rapport? Dans l’affirmative, quelles sont-elles? Puis-je recevoir une copie du rapport? Enfin,  qu’en est-il des dysfonctionnements dénoncés par les lanceurs d’alerte ? CRI N°17 Mme\"},\n",
              "  {'speaker': 'Valérie Glatigny',\n",
              "   'title': 'Ministre de l’Enseignement supérieur, de l’Enseignement de la Promotion sociale, des Hôpitaux universitaires, de l’Aide à la jeunesse, des Maisons de Justice, de la Jeunesse, des Sports et de la Promotion de Bruxelles',\n",
              "   'text': \", ministre de l’Enseignement supérieur, de l'Enseignement de promotion sociale, des Hôpitaux universitaires, de l’Aiïde à la jeunesse, des Maisons de justice, de la Jeunesse, des Sports et de la Promotion de Bruxelles. — Monsieur le Député, le rapport auquel vous faites référence a été fourni par un réviseur externe à ma demande, à la suite d’allégations concernant une mauvaise utilisation de subventions publiques. Je ne me prononcerai pas sur le conflit interpersonnel à ce sujet, qui prend d’ailleurs une tournure judiciaire puisqu’une partie a lancé des accusations de détournement de fonds, tandis que l’autre partie a porté plainte pour diffamation. Ce sera évidemment à la justice  de trancher.  Le réviseur externe n’a trouvé aucune trace d’un quelconque détournement de subventions publiques, mais il a formulé des recommandations: la présentation des comptes de la LBFA devrait être adaptée et les outils de contrôle de l'Administration générale du sport pourraient être améliorés. Bien entendu, il ne m’appartient pas d’orienter le travail du réviseur externe d’une quelconque manière. Par ailleurs, l’établissement d’un rapport intermédiaire et son envoi à l’organisme concerné constituent une pratique courante qui permet  aux responsables de répondre aux critiques formulées à leur égard.  En ce qui concerne le champ d'investigation, je rappelle que c’est bien l’utilisation des subventions publiques qui faisait l’objet d’un examen et non l'apport privé. La LBFA étant une ASBL, ses comptes doivent être soumis à l'approbation de son conseil d'administration; or, les administrateurs ont bien approuvé les comptes de 2022 et le budget de 2023. Toutefois, cet aspect devrait être approfondi pour l’avenir. Sachant qu’il s’agit du troisième audit après ceux de 2011 et de 2016, j'espère que celui-ci apportera de la sérénité dans les relations avec la LBFA, pour le bien de nos athlètes.  M.\"},\n",
              "  {'speaker': 'Mourad Sahli',\n",
              "   'title': 'Député',\n",
              "   'text': \". —Une fois que la justice est saisie du dossier, je comprends objectivement votre prudence, Madame la Ministre. Toutefois, je trouve anormal qu’un rapport intermédiaire soit communiqué à la partie auditée afin de recueillir son avis, d'autant plus que l’audit est réalisé par un organisme extérieur financé par de l’argent public.  À la veille des Jeux olympiques, je ne souhaite pas que cette situation ait un impact négatif sur nos sportifs qui doivent se concentrer sur leurs entraînements. [l me semble essentiel de tirer tout cela au clair et j’espère recevoir une copie de ce rapport. Après ces déboires, il s’agit de tourner la page pour en construire une nouvelle que j’espère meilleure.\"}]}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "questions[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_directory(directory_path, known_speakers):\n",
        "    all_questions_data = []\n",
        "    all_projets_data = []\n",
        "\n",
        "    for file_name in os.listdir(directory_path):\n",
        "        if file_name.endswith('.txt'):\n",
        "            file_path = os.path.join(directory_path, file_name)\n",
        "            questions_data, projets_data = process_file(file_path, known_speakers)\n",
        "            all_questions_data.extend(questions_data)\n",
        "            #all_projets_data.extend(projets_data)\n",
        "\n",
        "    create_json_file(all_questions_data, all_projets_data, 'combined.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "process_directory(r'C:\\Users\\Stephanie\\Documents\\GitHub\\NLP_Parlement\\txt', known_speakers)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To DO \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
