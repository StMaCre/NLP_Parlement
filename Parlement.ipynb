{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HdOJ8c8Y6ZGv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "from PyPDF2 import PdfReader\n",
        "from datetime import datetime \n",
        "import dateparser\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import cv2\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "from dateparser import parse as dateparser\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fZvnsUL28q9G"
      },
      "source": [
        "# Extracting the text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "V_7X4bAJ9Pca"
      },
      "outputs": [],
      "source": [
        "def extract_pdf_to_txt(src_dir, dest_dir):\n",
        "    # List all files in the source directory\n",
        "    all_files = os.listdir(src_dir)\n",
        "\n",
        "    # Filter PDF files\n",
        "    pdf_files = [f for f in all_files if f.lower().endswith('.pdf')]\n",
        "\n",
        "    # Loop through all the PDF files\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(src_dir, pdf_file)\n",
        "        reader = PdfReader(pdf_path)\n",
        "\n",
        "        # Extract text from the first page\n",
        "        first_page_text = reader.pages[0].extract_text()\n",
        "\n",
        "        # Search for a date in the format \"DD MONTH YYYY\" using a regular expression\n",
        "        date_match = re.search(r'\\d{1,2} (JANVIER|FÉVRIER|MARS|AVRIL|MAI|JUIN|JUILLET|AOÛT|SEPTEMBRE|OCTOBRE|NOVEMBRE|DÉCEMBRE)  \\d{4}', first_page_text)\n",
        "\n",
        "        # If a date is found, use it as a filename\n",
        "        if date_match:\n",
        "            # Get the matched date string\n",
        "            date_str = date_match.group()\n",
        "\n",
        "            # Remove extra spaces from the date string\n",
        "            date_str = ' '.join(date_str.split())\n",
        "\n",
        "            # Convert the date string to a datetime object\n",
        "            date = dateparser.parse(date_str, languages=['fr'])\n",
        "\n",
        "            # Format the date as \"YY-MM-DD\"\n",
        "            formatted_date = date.strftime('%y-%m-%d')\n",
        "\n",
        "            # Create a filename using the formatted date\n",
        "            filename = f\"{formatted_date}.txt\"\n",
        "        # If a date is not found, use the original PDF filename as a base\n",
        "        else:\n",
        "            filename = f\"{os.path.splitext(pdf_file)[0]}.txt\"\n",
        "\n",
        "        txt_path = os.path.join(dest_dir, filename)\n",
        "\n",
        "        # Open the output file for writing with 'utf-8' encoding\n",
        "        with open(txt_path, \"w\", encoding='utf-8') as f:\n",
        "            # Loop through all the pages in the PDF\n",
        "            for page in reader.pages:\n",
        "                # Extract text from the current page and write it to the output file\n",
        "                f.write(page.extract_text())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using cv2\n",
        "def extract_pdf_to_txt(src_dir, dest_dir):\n",
        "    pytesseract.pytesseract.tesseract_cmd = 'C:/Program Files/Tesseract-OCR/tesseract.exe'\n",
        "\n",
        "    all_files = os.listdir(src_dir)\n",
        "    pdf_files = [f for f in all_files if f.lower().endswith('.pdf')]\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(src_dir, pdf_file)\n",
        "\n",
        "        # Convert PDF pages to images\n",
        "        images = convert_from_path(pdf_path)\n",
        "\n",
        "        # Extract text from the first page\n",
        "        first_page_np = np.array(images[0])\n",
        "        first_page_gray = cv2.cvtColor(first_page_np, cv2.COLOR_BGR2GRAY)\n",
        "        first_page_thresh = cv2.threshold(first_page_gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
        "        first_page_text = pytesseract.image_to_string(first_page_thresh, lang='fra', output_type=pytesseract.Output.STRING)\n",
        "\n",
        "        date_match = re.search(r'\\d{1,2} (JANVIER|FÉVRIER|MARS|AVRIL|MAI|JUIN|JUILLET|AOÛT|SEPTEMBRE|OCTOBRE|NOVEMBRE|DÉCEMBRE) \\d{4}', first_page_text)\n",
        "\n",
        "        if date_match:\n",
        "            date_str = date_match.group()\n",
        "            date_str = ' '.join(date_str.split())\n",
        "            date = dateparser(date_str, languages=['fr'])\n",
        "            formatted_date = date.strftime('%y-%m-%d')\n",
        "            filename = f\"{formatted_date}.txt\"\n",
        "        else:\n",
        "            filename = f\"{os.path.splitext(pdf_file)[0]}.txt\"\n",
        "        \n",
        "        def preprocess_image(image_np):\n",
        "            # Resize the image\n",
        "            height, width = image_np.shape[:2]\n",
        "            resized_image = cv2.resize(image_np, (width * 2, height * 2), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "            # Convert to grayscale\n",
        "            gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Apply Gaussian blur\n",
        "            blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "            # Thresholding\n",
        "            thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
        "\n",
        "            # Dilation and Erosion\n",
        "            kernel = np.ones((1, 1), np.uint8)\n",
        "            dilated = cv2.dilate(thresh, kernel, iterations=1)\n",
        "            eroded = cv2.erode(dilated, kernel, iterations=1)\n",
        "\n",
        "            return eroded\n",
        "\n",
        "        txt_path = os.path.join(dest_dir, filename)\n",
        "\n",
        "        with open(txt_path, \"w\", encoding='utf-8') as f:\n",
        "            for image in images:\n",
        "                image_np = np.array(image)\n",
        "                preprocessed_image = preprocess_image(image_np)\n",
        "                text = pytesseract.image_to_string(preprocessed_image, lang='fra', output_type=pytesseract.Output.STRING)\n",
        "                f.write(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "src_dir = r'C:\\Users\\Stephanie\\Documents\\GitHub\\NLP_Parlement\\pdf'\n",
        "dest_dir = r'C:\\Users\\Stephanie\\Documents\\GitHub\\NLP_Parlement\\txt'\n",
        "extract_pdf_to_txt(src_dir, dest_dir)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uZRPFKQlA9NZ"
      },
      "source": [
        "# Important information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "parlementarians = pd.read_csv(r'C:\\Users\\Stephanie\\Documents\\GitHub\\NLP_Parlement\\2022.07.22_Parlementaires PFWB_0.csv', header=None)\n",
        "column_names = ['Prénom', 'nom', 'sexe', 'résidence', 'naissance', 'date', 'parti', 'titre']\n",
        "parlementarians.columns = column_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "known_speakers = parlementarians.apply(lambda row: {\"name\": row[\"Prénom\"] + \" \" + row[\"nom\"], \"title\": row[\"titre\"], 'parti': row['parti']}, axis=1).to_list()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EybferjhR1Hq"
      },
      "source": [
        "# Convert to JSON file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DbTAYxHpSKzf"
      },
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "# Define regular expressions and helper function to parse questions, speakers and statements\n",
        "def find_starting_point(text, starting_sentence='La séance est ouverte'):\n",
        "    starting_point = text.find(starting_sentence)\n",
        "    if starting_point != -1:\n",
        "        return starting_point + len(starting_sentence)\n",
        "    return None\n",
        "\n",
        "def clean_line_breaks(text):\n",
        "    # Replace hyphenated line breaks with an empty string\n",
        "    cleaned_text = re.sub(r'-\\s+', '', text)\n",
        "    # Clean the text from the \\n characters \n",
        "    cleaned_text = cleaned_text.replace('\\n', ' ')\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "def find_questions(text):\n",
        "    question_pattern = re.compile(r'(?<=\\d\\.\\d Question)(.*?)(?=\\d\\.\\d Question)', re.DOTALL)\n",
        "    return question_pattern.findall(text)\n",
        "\n",
        "def find_projets(text):\n",
        "    projet_pattern = re.compile(r'\\d+ Projet de décret(.*?)(?=\\d+ Projet de décret|$)', re.DOTALL)\n",
        "    return projet_pattern.findall(text)\n",
        "\n",
        "def process_transcript(text, known_speakers):\n",
        "    # Create a regex pattern for speaker names\n",
        "    speaker_pattern = r'(?:Mme|M\\.)\\s+(?:' + '|'.join([re.escape(speaker[\"name\"]) for speaker in known_speakers]) + r')\\b(?:\\s*\\([^)]*\\)|\\s*[,.-]|-)'\n",
        "\n",
        "\n",
        "    # Update the regex pattern to account for the space before the speaker's name\n",
        "    speaker_pattern_with_space = r'\\s*(?:' + '|'.join([re.escape(speaker[\"name\"]) for speaker in known_speakers]) + r')\\b'\n",
        "\n",
        "    # Find the starting point of the actual conversation\n",
        "    starting_point_pattern = re.compile(r'Question de .*? à .*?«.*?»', re.DOTALL)\n",
        "    starting_point = starting_point_pattern.search(text)\n",
        "    if starting_point:\n",
        "        text = text[starting_point.end():]\n",
        "\n",
        "    # Extract the theme\n",
        "    theme_pattern = re.compile(r'«(.*?)»')\n",
        "    theme_match = theme_pattern.search(text)\n",
        "    if theme_match:\n",
        "        theme = theme_match.group(1)\n",
        "        # Update the starting point of the text after the theme\n",
        "        text = text[theme_match.end():]\n",
        "    else:\n",
        "        theme = 'TBD'\n",
        "\n",
        "    # Split the text into parts\n",
        "    parts = re.split('(' + speaker_pattern + ')', text)\n",
        "    # Initialize the result list\n",
        "    result = []\n",
        "\n",
        "\n",
        "    # Initialize the current speaker\n",
        "    current_speaker = None\n",
        "\n",
        "    # Iterate through the parts\n",
        "    for part in parts:\n",
        "        found_speaker = False\n",
        "        for speaker in known_speakers:\n",
        "            if speaker[\"name\"] == part.strip():\n",
        "                current_speaker = speaker\n",
        "                found_speaker = True\n",
        "                break\n",
        "\n",
        "        if not found_speaker and current_speaker is not None:\n",
        "            # Append the spoken text by the current speaker\n",
        "            spoken_text = part.strip()\n",
        "\n",
        "            # Remove text within brackets and following whitespace\n",
        "            spoken_text = re.sub(r'\\(.*?\\)\\s*', '', spoken_text)\n",
        "\n",
        "            if spoken_text:\n",
        "                result.append({\"speaker\": current_speaker[\"name\"], \"title\": current_speaker[\"title\"], \"text\": spoken_text})\n",
        "\n",
        "    return result, theme\n",
        "\n",
        "def create_json_file(questions_data, projets_data, filename):\n",
        "    data = {\n",
        "        \"questions\": questions_data,\n",
        "        \"projets\": projets_data,\n",
        "    }\n",
        "    with open(filename, \"w\", encoding='utf-8') as outfile:\n",
        "        json.dump(data, outfile, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_file(file_path, known_speakers):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        full_text = f.read()\n",
        "\n",
        "    starting_point = find_starting_point(full_text)\n",
        "    if starting_point is not None:\n",
        "        transcript_text = full_text[starting_point:]\n",
        "    else:\n",
        "        print(\"Starting sentence not found. Analyzing the full text.\")\n",
        "        transcript_text = full_text\n",
        "\n",
        "    transcript_text = transcript_text.replace('\\n', ' ')\n",
        "    transcript_text = clean_line_breaks(transcript_text)\n",
        "\n",
        "    date = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "    # Find and process questions\n",
        "    questions_text = find_questions(transcript_text)\n",
        "    questions_data = []\n",
        "    for question_text in questions_text:\n",
        "        result, theme = process_transcript(question_text, known_speakers)\n",
        "        question_data = {\n",
        "            \"date\": date,\n",
        "            \"type\": 'question',\n",
        "            \"theme\": theme,\n",
        "            'thematic': 'TBD', \n",
        "            \"text\": result\n",
        "        }\n",
        "        questions_data.append(question_data)\n",
        "\n",
        "    # Find and process projets\n",
        "    projets_text = find_projets(transcript_text)\n",
        "    projets_data = []\n",
        "    for projet_text in projets_text:\n",
        "        result, theme = process_transcript(projet_text, known_speakers)\n",
        "        projet_data = {\n",
        "            \"date\": date,\n",
        "            \"type\": 'projet',\n",
        "            \"theme\": theme,\n",
        "            'thematic': 'TBD', \n",
        "            \"text\": result\n",
        "        }\n",
        "        projets_data.append(projet_data)\n",
        "\n",
        "    create_json_file(questions_data, projets_data, f\"{date}.json\")\n",
        "\n",
        "    return questions_data, projets_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "questions, projets = process_file(r'txt\\23-04-12.txt', known_speakers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'date': '23-04-12',\n",
              " 'type': 'question',\n",
              " 'theme': 'Audit externe de la Ligue belge francophone d’athlétisme',\n",
              " 'thematic': 'TBD',\n",
              " 'text': [{'speaker': 'Mourad Sahli',\n",
              "   'title': 'Député',\n",
              "   'text': \". Le 14 mars dernier, en commission des Sports, Madame la Ministre, je vous ai interrogée sur les dysfonctionnements au sein de la Ligue belge francophone d’athlétisme et, en particulier, sur les difficultés entre certains administrateurs et le président de la LBFA. Vous m'avez alors répondu que, d’après le rapport d’audit, la LBFA entretenait une comptabilité correcte et qu'elle s’inscrivait dans un processus de bonne gouvernance. Je  trouvais votre réponse quelque peu succincte.  À mes yeux, deux points sont importants dans ce dossier: d’une part, le rapport provisoire reçu par votre administration aurait été envoyé à la LBFA pour avoir son avis avant l'élaboration du rapport final et, d’autre part, les lanceurs d'alerte n’auraient pas été auditionnés et n’auraient, par conséquent, pas pu  donner leur avis.  N’est-il pas étrange que ce rapport d’audit provisoire soit remis à la partie qui fait l’objet d’un audit afin d’obtenir son avis? Cela ne signifie-t-il pas de lui demander une autoévaluation? Cela me semble contraire au but d’un audit externe et indépendant? Disposez-vous des conclusions de ce rapport? Dans l’affirmative, quelles sont-elles? Puis-je recevoir une copie du rapport? Enfin,  qu’en est-il des dysfonctionnements dénoncés par les lanceurs d’alerte ? CRI N°17 Mme\"},\n",
              "  {'speaker': 'Valérie Glatigny',\n",
              "   'title': 'Ministre de l’Enseignement supérieur, de l’Enseignement de la Promotion sociale, des Hôpitaux universitaires, de l’Aide à la jeunesse, des Maisons de Justice, de la Jeunesse, des Sports et de la Promotion de Bruxelles',\n",
              "   'text': \", ministre de l’Enseignement supérieur, de l'Enseignement de promotion sociale, des Hôpitaux universitaires, de l’Aiïde à la jeunesse, des Maisons de justice, de la Jeunesse, des Sports et de la Promotion de Bruxelles. — Monsieur le Député, le rapport auquel vous faites référence a été fourni par un réviseur externe à ma demande, à la suite d’allégations concernant une mauvaise utilisation de subventions publiques. Je ne me prononcerai pas sur le conflit interpersonnel à ce sujet, qui prend d’ailleurs une tournure judiciaire puisqu’une partie a lancé des accusations de détournement de fonds, tandis que l’autre partie a porté plainte pour diffamation. Ce sera évidemment à la justice  de trancher.  Le réviseur externe n’a trouvé aucune trace d’un quelconque détournement de subventions publiques, mais il a formulé des recommandations: la présentation des comptes de la LBFA devrait être adaptée et les outils de contrôle de l'Administration générale du sport pourraient être améliorés. Bien entendu, il ne m’appartient pas d’orienter le travail du réviseur externe d’une quelconque manière. Par ailleurs, l’établissement d’un rapport intermédiaire et son envoi à l’organisme concerné constituent une pratique courante qui permet  aux responsables de répondre aux critiques formulées à leur égard.  En ce qui concerne le champ d'investigation, je rappelle que c’est bien l’utilisation des subventions publiques qui faisait l’objet d’un examen et non l'apport privé. La LBFA étant une ASBL, ses comptes doivent être soumis à l'approbation de son conseil d'administration; or, les administrateurs ont bien approuvé les comptes de 2022 et le budget de 2023. Toutefois, cet aspect devrait être approfondi pour l’avenir. Sachant qu’il s’agit du troisième audit après ceux de 2011 et de 2016, j'espère que celui-ci apportera de la sérénité dans les relations avec la LBFA, pour le bien de nos athlètes.  M.\"},\n",
              "  {'speaker': 'Mourad Sahli',\n",
              "   'title': 'Député',\n",
              "   'text': \". —Une fois que la justice est saisie du dossier, je comprends objectivement votre prudence, Madame la Ministre. Toutefois, je trouve anormal qu’un rapport intermédiaire soit communiqué à la partie auditée afin de recueillir son avis, d'autant plus que l’audit est réalisé par un organisme extérieur financé par de l’argent public.  À la veille des Jeux olympiques, je ne souhaite pas que cette situation ait un impact négatif sur nos sportifs qui doivent se concentrer sur leurs entraînements. [l me semble essentiel de tirer tout cela au clair et j’espère recevoir une copie de ce rapport. Après ces déboires, il s’agit de tourner la page pour en construire une nouvelle que j’espère meilleure.\"}]}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "questions[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_directory(directory_path, known_speakers):\n",
        "    all_questions_data = []\n",
        "    all_projets_data = []\n",
        "\n",
        "    for file_name in os.listdir(directory_path):\n",
        "        if file_name.endswith('.txt'):\n",
        "            file_path = os.path.join(directory_path, file_name)\n",
        "            questions_data, projets_data = process_file(file_path, known_speakers)\n",
        "            all_questions_data.extend(questions_data)\n",
        "            #all_projets_data.extend(projets_data)\n",
        "\n",
        "    create_json_file(all_questions_data, all_projets_data, 'combined.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "process_directory(r'C:\\Users\\Stephanie\\Documents\\GitHub\\NLP_Parlement\\txt', known_speakers)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Thematic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "import classy_classification\n",
        "from spacy.util import minibatch, compounding\n",
        "from spacy.training.example import Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = pd.read_csv('labels.csv')\n",
        "labels['combined_text'] = labels['titre'] + ' ' + labels['text']\n",
        "\n",
        "# Convert your data to the required format\n",
        "def load_data(df):\n",
        "    data = []\n",
        "    for index, row in df.iterrows():\n",
        "        label = row['label']\n",
        "        text = row['combined_text']\n",
        "        data.append((text, {\"cats\": {label: 1.0}}))\n",
        "    return data\n",
        "\n",
        "train_data = load_data(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 3.4668735414743423\n",
            "Epoch: 1, Loss: 3.5999881103634834\n",
            "Epoch: 2, Loss: 3.475020319223404\n",
            "Epoch: 3, Loss: 3.3965729102492332\n",
            "Epoch: 4, Loss: 3.2497308775782585\n",
            "Epoch: 5, Loss: 2.9026045463979244\n",
            "Epoch: 6, Loss: 2.2346629481762648\n",
            "Epoch: 7, Loss: 1.5175540847267257\n",
            "Epoch: 8, Loss: 0.7579404590360355\n",
            "Epoch: 9, Loss: 0.41725427449591734\n",
            "{'Budget': 0.019625067710876465, 'Culture et média ': 0.3861103653907776, 'Enfance': 0.0010101201478391886, 'Enseignement supérieur et Promotion Sociale ': 0.014809043146669865, 'Interculturalité et Egalité des Chances ': 0.01695610210299492, 'Jeunesse et Sport ': 0.03689073771238327, 'Politique et International': 0.011693249456584454, 'Santé': 0.5129052996635437}\n"
          ]
        }
      ],
      "source": [
        "# Create the blank model and add the text categorizer to the pipeline\n",
        "nlp = spacy.blank(\"fr\")\n",
        "textcat = nlp.add_pipe(\"textcat\")\n",
        "\n",
        "# Add labels to the text categorizer\n",
        "for _, annotations in train_data:\n",
        "    for label in annotations.get(\"cats\"):\n",
        "        textcat.add_label(label)\n",
        "\n",
        "# Train the text categorizer\n",
        "optimizer = nlp.begin_training()\n",
        "batch_size = 8\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    losses = {}\n",
        "    batches = minibatch(train_data, size=compounding(1.0, 4.0, 1.001))\n",
        "    \n",
        "    for batch in batches:\n",
        "        texts, annotations = zip(*batch)\n",
        "        examples = [Example.from_dict(nlp.make_doc(text), annots) for text, annots in zip(texts, annotations)]\n",
        "        nlp.update(examples, sgd=optimizer, drop=0.2, losses=losses)\n",
        "    print(f\"Epoch: {epoch}, Loss: {losses['textcat']}\")\n",
        "\n",
        "# Test the trained model\n",
        "test_text = \"Réduction de la subvention accordée à bpost pour la distribution de la presse périodique\"\n",
        "doc = nlp(test_text)\n",
        "print(doc.cats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \" Madame la Ministre, nous ne parlons pas souvent de littérature dans ce Parlement et pourtant, en tant que romanistes, nous avons ce centre d’intérêt en commun. Je ne vous interroge pas sur la qualité, bien réelle, des œuvres produites en Fédération Wallonie-Bruxelles, mais plutôt sur les conditions socio-économiques dans lesquelles les auteurs de notre Fédération  doivent travailler.  Une étude récente réalisée par l’ASBL Bela jette une lumière assez crue sur ces conditions. Elle révèle notamment que trois quarts des auteurs vivent avec un revenu inférieur à 1 000 euros par mois tirés de leur activité. Ce n’est pas telle’ D . . . . D D ment étonnant lorsqu'on sait que, sur un livre vendu une vingtaine d’euros, l’auteur n’en reçoit que 1,5 voire maximum 2 euros. Il faut déjà en vendre beaucoup pour pouvoir en vivre. [l y a certes quelques best-sellers, mais ils font figure d’exceptions et sont les arbres qui cachent la forêt.  Au-delà de ces situations socio-économiques, l’étude révèle également un manque de reconnaissance et de considération ressenti par les personnes interrogées. Plus grave encore, elle fait apparaître une situation de détresse psychologique dans 7 % des cas. Sans vouloir réactiver l’image mythique du poète maudit ou de l’écrivain au ban de la société, le constat qui doit être posé ici et maintenant est bel et  bien celui d’une extrême précarité. CRI N°12 Madame la Ministre, comment réagissez-vous vis-à-vis de ce constat”? Quelles réponses pouvez-vous apporter à cette situation? Trois demandes sont clairement exprimées par les auteurs: d’abord celle d’un refinancement, le financement actuel étant jugé insuffisant; ensuite, celle d’un accompagnement d'ordre psychologique, pour aider les auteurs et autrices à sortir de l’isolement qu'ils et elles ressentent; enfin, celle d’un accompagnement plus professionnel pour leur permettre entre autres de bénéficier de plus de visibilité.  Quelles sont les initiatives nouvelles en matière de promotion des lettres belges de langue française? Je rappelle que c’est l’une des missions de notre Communauté depuis sa création. Quelle est, d’après les informations vous revenant du terrain, votre appréciation de la mise en œuvre de la réforme du statut des artistes entrée en application? Pensez-vous qu’elle constitue une réponse suffisante à  cette situation d’extrême précarité de nos auteurs?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Budget': 0.0012807386228814721, 'Culture et média ': 0.9656847715377808, 'Enfance': 0.0018391121411696076, 'Enseignement supérieur et Promotion Sociale ': 0.0018180167535319924, 'Interculturalité et Egalité des Chances ': 0.0018495387630537152, 'Jeunesse et Sport ': 0.01825811341404915, 'Politique et International': 0.006519528105854988, 'Santé': 0.002750222571194172}\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(text)\n",
        "print(doc.cats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# predicting the thematic \n",
        "def process_json(json_data, nlp):\n",
        "    questions = json_data[\"questions\"]\n",
        "    results = []\n",
        "\n",
        "    for question in questions:\n",
        "        theme = question[\"theme\"]\n",
        "        text_parts = question[\"text\"]\n",
        "\n",
        "        for text_part in text_parts:\n",
        "            text = text_part[\"text\"]\n",
        "            words = text.split()\n",
        "            if len(words) > 10:\n",
        "                # Get the predicted categories and their scores\n",
        "                doc = nlp(text)\n",
        "                categories = doc.cats\n",
        "\n",
        "                # Find the category with the highest score\n",
        "                max_score = 0\n",
        "                best_category = None\n",
        "                for category, score in categories.items():\n",
        "                    if score > max_score:\n",
        "                        max_score = score\n",
        "                        best_category = category\n",
        "                \n",
        "                # Update the 'thematic' field in the question\n",
        "                question[\"thematic\"] = best_category\n",
        "\n",
        "                # Store the result\n",
        "                '''results.append({\"theme\": theme, \"text\": text, \"predicted_thematic\": best_category})\n",
        "                break\n",
        "                '''\n",
        "\n",
        "    return json_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the JSON data\n",
        "with open(\"json\\combined.json\", \"r\", encoding='utf-8') as f:\n",
        "    json_data = json.load(f)\n",
        "\n",
        "# Process the JSON data\n",
        "updated_json_data = process_json(json_data, nlp)\n",
        "\n",
        "# Write the updated JSON data back to the file\n",
        "with open(\"json\\combined.json\", \"w\", encoding='utf-8') as f:\n",
        "    json.dump(updated_json_data, f, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of questions: 119\n"
          ]
        }
      ],
      "source": [
        "# How Many questions in my json file \n",
        "with open(r\"C:\\Users\\Stephanie\\Documents\\GitHub\\NLP_Parlement\\json\\combined.json\", \"r\", encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Count the number of questions\n",
        "question_count = len(data['questions'])\n",
        "\n",
        "print(\"Number of questions:\", question_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Date: 20-05-13, Number of questions: 8\n",
            "Date: 22-09-08, Number of questions: 8\n",
            "Date: 22-09-14, Number of questions: 6\n",
            "Date: 22-09-28, Number of questions: 7\n",
            "Date: 22-10-12, Number of questions: 8\n",
            "Date: 22-10-26, Number of questions: 8\n",
            "Date: 22-11-16, Number of questions: 6\n",
            "Date: 22-11-30, Number of questions: 8\n",
            "Date: 22-12-14, Number of questions: 8\n",
            "Date: 23-01-11, Number of questions: 8\n",
            "Date: 23-01-25, Number of questions: 8\n",
            "Date: 23-02-08, Number of questions: 7\n",
            "Date: 23-03-01, Number of questions: 8\n",
            "Date: 23-03-15, Number of questions: 8\n",
            "Date: 23-03-29, Number of questions: 7\n",
            "Date: 23-04-12, Number of questions: 6\n"
          ]
        }
      ],
      "source": [
        "# How many questions per date\n",
        "# Count the number of questions per date\n",
        "questions_per_date = {}\n",
        "for question in data[\"questions\"]:\n",
        "    date = question[\"date\"]\n",
        "    if date in questions_per_date:\n",
        "        questions_per_date[date] += 1\n",
        "    else:\n",
        "        questions_per_date[date] = 1\n",
        "\n",
        "# Print the number of questions per date\n",
        "for date, count in questions_per_date.items():\n",
        "    print(f\"Date: {date}, Number of questions: {count}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
